# PyTorch 사용

### Deep Learning coding의 전체적인 flow

1. Pre-Processing the data
2. Set Hyperparameter
3. Generate Custom dataset and data class
4. Generate Neural Network
5. Train and Test the model
6. Visualize the result and save/load model



### 필수적인 module

```python
import torch				# 기본적인 torch module
import torch.nn as nn		# layer나 modeling에 대한 함수
import torch.functional as F	# 활성화 함수와 같은 단순한 수식 포함
								# functional 내에는 class가 아니라 함수가 있어서 import 후 사용할 때 따로 객체를 								    # 만들지 않아도 사용할 수 있다!
import torch.optim as optim	# 다양한 optimizer들

import torch.utils.data as data_util	# TensorDataset, DataLoader등등의 dataset 관련 module
# or
from torch.utils.data import TensorDataset, DataLoader 

#-----------------위 module들은 거의 외우기-----------------

import numpy	# data analysis tool
import matplotlib.pyplot as plt	# data visualization tool
import pandas	# dataset manage tool
import sklearn.model_selection import train_test_split	# data에서 train, test 분리할 때 불러오는 함수 
import tqdm.notebook import tqdm	# 진행상황 bar 생성

import opencv # image data 관리
```



### 전체적인 process

#### 1. Pre-Processing the data

- 결측치를 확인한다
- Entry들간 상관관계를 check한다.
- 필요없는 결측치들은 지우고 나중에 사용될 결측치들은 상관관계를 바탕으로 값을 채워준다.
- `Pickle`, `Feather`, `hdf5`와 같은 형식으로 data를 저장한다.



#### 2. Set Hyperparameter

- `argparse`를 이용하여 parameter들을 지정한다. 
- data
  - scaling 하기
  - 어떤 entry들을 사용할지 
  - 계산의 Precision 정하기
- Model
  - RNN, CNN과 같이 model의 구조 결정하기
  - hidden layer수 결정하기
  - batch normalization을 사용할 것인지 결정
  - Dropout rate
- Training 
  - Loss Fuction
  - Optimizer
  - Learning rate
  - Early Stop 유무
  - Regularization 결정

```python
epoch = 10
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr = 1e-2, momentum = 0.9)
.
.
.
```

\* `model.parameters()`: module parameter에 대한 iterator를 반환하며 보통 optimizer에 넘겨줄 때 말고는 쓰지 않는다.

#### 3. Generate Custom dataset and data class

- Data Class
  - Hyperparameter들 load하기
  - Pre Processed data 읽기
  - train, validation, test data 나누기
  - 기존의 dataset을 `torch.Dataset`으로 변환
  - `torch.Dataset`을 `torch.DataLoader`로 변환하도록 함
- Dataset Class
  - `torch.Dataset`을 상속받아서 custom dataset을 만들 수 있다.
  - dataset style
    - `__init__(self)`: preparation
    - `__len__(self)`: Dataset 총 길이
    - `__getitem__(self, idx)`: i번째 data return



```python
class DatasetName(Dataset):	# torch.Dataset format(by inheritting)
    def __init_(self, etc.)	# input data shape, output data shape, feature num
    	self.a = ~~
        self.b = ~~
        
    def __len__(self):
        return len(~~~)
    
    def __getitem__(self, idx):
        x = ~~~ # x data
        y = ~~~ # y data(label)
        
        return x, y
```

- dataloader 

Custom dataset, batch_size, shuffle유무, num_worker(gpu core(확실하지 않음)를 연산에 얼만큼 사용할 것인지 결정), 등등의 parameter를 입력받아 data loader 객체를 만든다. 

```python
train_loader = DataLoader(self.train_dataset,
						  batch_size = batch_size,
                          shuffle = True
                          num_workers = num_workers)
```



#### 4. Generate Neural Network

- nn.Module을 상속해야한다.
- `__init__()`과 `forward()`를 override 해야 함
- `__init__()의 역할`
  - layer 혹은 activation fuction들을 정의한다.
- `forward()의 역할`
  - `__init__()`에서 정의한 layer나 activation function들을 활용하여 input이 어떻게 output으로 나올지를 정의해준다.

```python
class nnName(nn.Module):	# nn.Module을 상속 받아서 nn.Module내부 필요한 method들을 활용할 수 있게 한다.
	def __init__(self, etc.):	# input_shape, output_shape, num_layer..
        super(nnName, self).__init__()
        self.module1 = ...
        self.module2 = ...
        '''
        ex) 
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)
        '''
       
    def forward(self, x):
        x = function1(x)
        x = function2(x);
        .
        .
        .
        '''
        init에 정의된 attr로 forward 구성
        x = F.relu(self.conv1(x)) 
        '''
        return x        
```



#### 5. Train and Test the model

- Train 과정

```python
model.train()
for epoch in range(epoch): 	# 이전에 정의한 epoch 만큼 학습
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        output = model(x)	# Feed Forwarding

        ~~~ model

        loss = loss_fuct(output, y)	# 사용자가 지정한 loss fuction에 예측값과 정답값을 넣어 loss 계산

        # Back Propagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Calculate loss
        train_loss += loss.item()
    train_loss /= len(train_dataset 총 길이)
    train_loss_list[epoch] = train_loss	# store train_loss to visualize
```

- Test 과정

```python
model.test()
with torch.no_grad():
	for i, (x, y) in enumerate(test_loader):
        x, y = x.to(device), y.to(device)
        
        # feed forwarding 
        output = model(x)
        
        prediction
        
        
        
```

\* `model.eval()` ,`model.train()`

: 모델 내부에 Dropout 혹은 BatchNormalization이 있는 model은 모델을 학습시킬 때와 평가할 때가 다르기 때문에 반드시 이를 명시해야 한다.



#### 6. Visualize the result and save/load model

-  visualization 

이전에 Train과 Test 과정에서 저장했던 loss값들의 array들을 matplot 툴을 통해 시각화 한다.

- save / load model

https://tutorials.pytorch.kr/beginner/saving_loading_models.html 사이트 참조

`torch.save` method를 통해 model 저장(관례상 확장자를 .pt나 .pth로 저장)

`torch.load` method를 통해 model 불러옴

```python
Model = model(*args, **kwargs)
# python 내 state_dict라는 내장함수를 통해 model, optimizer, loss func, 등등의 값을 저장한다.
torch.save({'model_state_dict' : model.state_dict(),
            'optimizer_state_dict' : optimizer.state_dict(),
            .
            .
            .
           },
          torch model save file path)

# --------------
new_model = model(*args, **kwargs)
new_optimizer = optim.Adam(*args, **kwargs)

checkpoint = torch.load(FILE_PATH)
new_model.load_state_dict(checkpoint['model_state_dict'])
```

\* `state_dict()`와 `load_state_dict()`에 대하여

- `state_dict()`: 모델의 모든 상태를 dictionary 형태로 변환
- `load_state_dict(state_dict, strict = True)`: 모델의 모든 상태를 새로운 모델로 복사하며 `strict = True`일 경우 module의 이름이 정확히 같아야함



# Pytorch 사용시 마주칠 수 있는 문제, 헷갈리는 pytorch 문법

### import 관련

- 같은 폴더 내 python file 모듈을 불러올 때 

  `import python_file_name as pf` => pf.~~~로 module내 class 들을 사용할 수 있음

  `from python_file_name import class_name(or function_name)` => 특정 모듈에서 원하는 class나 함수만 불러올 때

- 다른 폴더 내 python file 모듈을 불러올 때

  `sys.path.append("folder path")`(folder path: 다른 폴더의 경로)를 적고 이후에 폴더 내 원하는 module을 불러온다.

### *args, **kwargs

- *args 사용법

```python
def F(*Parmas):
	for i in (Params):
        print(i)
F('a', 'b', 'c')
out: 
  	a
    b
    c
```

: 여러 개의(복수개의) 인자를 함수로 받고자 할 때 쓰이며 tuple 형태로 입력을 받는다. 

*args는 변수를 몇개 받을 지 모르기 때문에 일반 변수보다 뒤에 위치해야 한다.



- **kwargs 사용법

```python
def F(**kwargs):
    for k, v in kwargs.items():
        print("{}, {}".format(k, v))
F(A = 'a', B = 'b')
out:
    A, a
    B, b
```

(키워드 = 특정 값) 형태로 함수를 호출할 수 있다.

함수로 전달될 때 dixt 형태로 전달



- **kwargs와 *args를 동시에 사용하기 위해서는 순서를 지켜야 한다.
- 일반변수 => *args => *kwargs 순서! (순서가 바뀌면 error가 난다.)

### pytorch의 Container

여러 layer들을 하나로 묶는 데 쓰인다.

ex)

>nn.Module
>
>nn.Sequential
>
>nn.ModuleList...

nn.Sequential 사용하는 방법

```python
model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )
"""
이 경우 model(x)는 nn.ReLU(nn.Conv2d(20,64,5)(nn.ReLU(nn.Conv2d(1,20,5)(x))))와 같음.
"""
```

이런 식으로 여러개의 layer들을 하나로 묶어줄 수 있다.



### 자주 쓰이는 Pytorch Layer의 인자

- `nn.Conv2d`의 parameter(`nn.Conv2d(3, 32, 3, padding = 1)`)

첫번째 param: input_channel_size이며 처음 layer에서는 주로 RGB depth인 3이 된다.

두번째 param: output_volume_size이며 Conv를 거쳐 Feature map의 개수 결정

세번째 param: kernel_size이며 Filter의 size 결정, 만약 3이면 3x3 filter를 사용하는 것

네번째 param: padding 여부와 padding size 결정, padding = 1이면 1만큼의 padding을 준다는 뜻

다섯번째 param: stride를 결정, 따로 주지 않으면 default로 stride = 1 속성이 지정됨



























